---
title: 'Data management protocol'
author: "N** B**"
date: "date of submission"
output:
  pdf_document: default
  html_document: default
  html_notebook:
    toc: yes
    toc_float: yes
    number_section: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# more internal settings can go here
# Consider help pages like:
# https://rmarkdown.rstudio.com/lesson-1.html
# https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf

```

### Loaded packages

```{r libraries, results=F}
# Load packages
require(RPostgreSQL)
require(getPass)
library(tidyverse)
library(sf)
library(lubridate)
library(zoo) # to extend the apply family of functions (rolling window)
```

### Preprocessing / R functions

```{r preprocessing, message=FALSE, warning=FALSE}
# If data preprocessing is needed, do it here.
# functions:
flagR <- function(x) { # function to flag values in quality check 1
  ifelse(x>=1, 1,0)    # sets flag to 0 when passed, to 1 when failed
}

flagP <- function(x){  # function flags values in quality check 3 (variability)
  ifelse(length(unique(x)) == 1, 1,0)  # no variability -> 1 unique val
}                      # must be applied using a rolling window 

```

# 1. Hobo meta informations

Hobo picture and description

![image of my Hobo](./10350049.jpg){width=250px}


# 2. Consistent HOBO data file
Read in from GitHub
```{r raw_data, eval=T}
# Load data from Github (then eval = TRUE)
my_HOBO <- read.csv("https://raw.githubusercontent.com/data-hydenv/data/master/hobo/2023/raw/10350049.csv", skip = 1)
# Load calibration file from GitHub
t_cal <- read.csv("https://raw.githubusercontent.com/data-hydenv/data/master/hobo/2023/calibration.csv")
```
Date/Time column must be parsed properly:
```{r parse datetime}
my_HOBO$Datum.Zeit..GMT.01.00 <- mdy_hms(my_HOBO$Datum.Zeit..GMT.01.00)

```

Tidy up and rename the columns
```{r tidy_cols}
my_HOBO <- as_tibble(my_HOBO) %>% 
  select(., 1:4) %>% 
  rename(id = 1, dttm = 2, temp = 3, lux = 4)

head(my_HOBO)
```


## 2.1. Calibration
The calibration data from GitHub is a table of the time when the calibration measurement was performed and the target value temperature. 

```{r calibration}
caltime <- t_cal$This.is.the.calibration.value[3] # time of calibration

calib_line <- which(my_HOBO$dttm == caltime) # according line in raw data

meas_temp <- num(my_HOBO[calib_line,]$temp, digits = 3) # what the HOBO measured

meas_temp <- meas_temp[1] # as non-numbered number

tru_temp <- num(as.numeric(t_cal$to.calibrate.your.Hobo.measurements.in..C.[3]),
                digits = 3) # what the target value is

tru_temp <- tru_temp[1] # target value as non-numbered number

cal_offset <- meas_temp-tru_temp # difference of the two

cal_offset <- as.numeric(cal_offset[1])

# so -0.267 is now my calibration offset

my_HOBO$temp <- my_HOBO$temp-cal_offset # subtract offset from raw temperature
```


## 2.2. create HOBO data file
Truncate data to the given time frame. First declare the target start and end
time using ´lubridate::ymd_hms´.
```{r truncate raw data}
start_time <- ymd_hms("2022-12-01 00:00:00") # start point
end_time <- ymd_hms("2023-01-07 23:50:00") # end point
time_range <- interval(start_time, end_time) # may be useful later
```
Filter the raw data to clip it according to the start and end times and also
overwrite the id column with a new value starting from `1`. `format()` was used
to format decimals in the light intensity and temperature vectors and to change
the timestamp format.
```{r format hobo file}
my_HOBO <- my_HOBO %>% 
  filter(between(dttm, start_time, end_time)) # filter 
my_HOBO <- my_HOBO %>% 
  mutate(., id = c(1:length(my_HOBO$id))) # and fix ID col

my_HOBO$temp <-  format(my_HOBO$temp, digits=3, nsmall=3) # fix decimals in temp
my_HOBO$lux <-  format(my_HOBO$lux, digits=3, nsmall=3) # and in light intensity

my_HOBO$dttm <- format(my_HOBO$dttm, "%Y-%m-%d %H:%M") # timestamp format
```
The `tibble: my_HOBO` was then written to disk and uploaded to GitHub.
```{r write hobo file}
# write to file
write_csv(my_HOBO, file = "10350049.csv")
```

## 2.3. verify your file

# 3. Quality control
Read the data back in from GitHub...
```{r formated_data}
# Load data from Github (then eval = TRUE)
HOBO_qc <- read.csv("https://raw.githubusercontent.com/data-hydenv/data/master/hobo/2023/10_minutes/10350049.csv")
head(data)

```
## 3.1. Quality control procedures (QCPs)

### 3.1.1. Measurement range (Plausible values)
...and check both value ranges.
```{r qcp1, eval=T}
# TODO NAs
range(HOBO_qc$temp)
range(HOBO_qc$lux)
```

**Question**: How many data points are outside the measurement range?

**Answer**: None

### 3.1.2. Plausible rate of change

```{r qcp2}
# lag one column, then mutate difference to new column
HOBO_wip <- HOBO_qc %>% 
  mutate(., lagged=lag(temp)) %>% 
  mutate(., delta=temp-lagged)

range(na.omit(HOBO_wip$delta)) # delta values out of legal range?
```

**Question**: Describe shortly how many data points failed during this QCP and discuss whether there is a certain daytime pattern of failure or not?

**Answer**:

```{r flag qcp2}
qc_R <- sapply(HOBO_wip$delta,flagR) # apply flagging function 

HOBO_wip <- cbind(HOBO_wip,qc_R) # collect results in new data frame

length(which(qc_R==1)) # count occurence of bad data points
# TODO day/night pattern?
```

### 3.1.3. Minimum variability (Persistence)
Use `zoo::rollapply()` to run the persistence check over a moving time window of 
6 time steps (i.e. 60 minutes).
```{r, flag qcp3}
qc2 <- rollapply(HOBO_wip$delta, width=6,FUN=flagP) # width parameter is window
```

**Task**: Code in this section should analyse the persistence.

```{r persistence}
# TODO
```

Compile results

```{r qc3 results}
qc_P <- c(1:length(HOBO_wip$delta)) # vector to store persistence flag
qc_P[1:5] <- NA # manually fill positions 1 through 5
qc_P[6:length(qc_P)] <- qc2 # append results
HOBO_wip <- cbind(HOBO_wip,qc_P) # collect in tibble
```


### 3.1.4. Light intensity
The HOBO was situated in a planter at all times so there was no disturbance of
the light measurement.
```{r qcp4}
hist(HOBO_wip$lux, breaks = 25)
summary(HOBO_wip$lux)
sd(HOBO_wip$lux)
toplux <- filter(HOBO_wip,lux<=500 & lux>1)
hist(toplux$lux,breaks=50)
```

Assign SICs from SOURCE!!! to light measurement data points
```{r assign SIC}
HOBO_wip <- HOBO_wip %>% 
  mutate(SIC = case_when(lux <= 10 ~ 'Night_0',
                         lux <= 500 ~ 'Rise_Set_1',
                         lux <= 2000 ~ 'Overcast_full_2',
                         lux <= 15000 ~ 'Overcast_light_3',
                         lux <= 20000 ~ 'Clear_4',
                         lux < 50000 ~ 'Sunshine_5',
                         lux >= 50000 ~ 'Brightshine_6',
                         ))

unique(HOBO_wip$SIC) # only the first four SICs present
```
**Task**: Discuss shortly how often and when during daytime the QCP4 flags bad data. Elaborate on some reasons for your results.

**Answer**:
```{r flag summary}
# analyse occurence of flags and aggregate qc fails 
HOBO_wip <- HOBO_wip %>% 
  mutate(qc_tot = qc_P + qc_R)

HOBO_wip$qc_tot[1:5] <- 0


HOBO_wip <- HOBO_wip %>% 
  mutate(qc_all = case_when(qc_tot == 0 ~ 0,
                   qc_tot != 0 ~ 1))

# as percentage

qc_result <- table(HOBO_wip$qc_all)

qc_result


num(((qc_result[2]/length(HOBO_wip$qc_all))*100),digits=4)
# TODO
# Present a table or graph to show how many data points fail during the four specific QCPs. Discuss shortly
# the reasons for failure and compare the different QCPs against each other.
# create qc_df
# TODO 
qc_df <- as_tibble(HOBO_wip)

# numbering all hours
hour_ct <- rep(c(1:912), each = 6)
# stick it to the df
qc_df <- cbind(qc_df, h_ct = hour_ct)

# qc_df <- qc_df %>% 
#   mutate(flag_)

table(qc_df$qc_tot)
# I have 203 cases but each one has failed only one of the quality checks

test <- qc_df %>% 
  group_by(., h_flag = h_ct) %>% 
  summarize(., sum_flags=sum(qc_all)) %>% 
  filter(sum_flags>=2)
# this yields a vector of all hours that must be set to NA
bad_hours <- test$h_flag

qc_df2 <- qc_df

bad_temps <- which(qc_df2$h_ct %in% bad_hours)
qc_df2$temp[bad_temps] <- NA
```
Share of NAs
```{r NA share}
# Calculate the share of NAs in your hourly time-series in % and write it into the
# meta table (sheet: “Quality


hrly_dat <- qc_df2 %>% 
  group_by(., h_ct) %>% 
  summarise(., mean_temp=mean(temp))
# this yields hourly means with NAs 
# share of NA in hourly time series

table(is.na(hrly_dat$mean_temp))
# 50 NAs

num((50/length(hrly_dat$mean_temp))*100,digits = 4)

```


## 3.2. Summarize

```{r summarize}
# code for summarizing here
# TODO
```

**Task**: Present a table or graph to show how many data points fail during the four specific QCPs. Discuss shortly the reasons for failure and compare the different QCPs against each other.

**Answer**:

**Task**: At the end of the code section above you should generate one! tibble or data.frame named `qc_df` with all time information, all data points (temperature and lux) and your outcomes of the different QCPs.

**Answer**:

## 3.3. Aggregate

```{r agg}
# TODO separate aggregation chunk
```


**Task**: At the end of the code section above you should generate one! tibble or data.frame named `hobo_hourly` with averaged temperature values per hour or NA values (if the hour is flagged as bad data). See exercise description for more details.

```{r write qc file}
dttm <- seq(start_time, end_time, by= "hours")

hobo_hourly <- hrly_dat %>% 
  select(th=mean_temp) %>% 
  mutate(date_time=dttm) %>% 
  mutate(origin=rep("H"))

hobo_hourly$date_time <- format(hobo_hourly$date_time, "%Y-%m-%d %H:%M:%S")
hobo_hourly$th <-  format(hobo_hourly$th, digits=3, nsmall=3)

write_csv(hobo_hourly, file = "10350049_Th.csv" )

```


# 4. Fill-up with reference station

## 4.1. reference station

```{r ref station}

# fill up all the NA in your
# hobo_hourly from chapter 3.3 based on a regression model between your station
# and a reference station. 

# WBI Station
# data import from hdd
WBI_raw <- read.csv("Stunde_096.csv", sep=";")
# convert to proper POSX or ld object
WBI_raw$Tag <- dmy(WBI_raw$Tag)
class(WBI_raw$Tag)
# works semi well with this timestamp...
WBI_raw$Stunde <- hm(WBI_raw$Stunde)
class(WBI_raw$Stunde)

# i'll make my own
# declare time frame
start_time <- ymd_hms("2022-12-01 00:00:00")
end_time <- ymd_hms("2023-01-07 23:50:00")
time_range <- interval(start_time, end_time)
# make my own dttm and scrap the ones that came with the WBI data
dttm <- seq(start_time, end_time, by= "hours")
# clip WBI data, add dttm and rename cols
Wdat_WBI <- WBI_raw %>% 
  filter(.,between(Tag, date(start_time), date(end_time))) %>% 
  select(.,temp=AVG_TA200) %>% 
  mutate(dttm=dttm)
# TODO format the timestamp to get rid of seconds eventually (if the assignment says so)


# FREIBURG Garten station
GAR_raw <- read.csv("Freiburg_Garten_2022-11-30_2023-01-08.csv")

# correct missing line in GAR_raw
GAR_raw <- GAR_raw %>% 
  add_row(UTC = "2022-12-16 10:00:00", Lokalzeit = "2022-12-16 11:00:00", Lufttemperatur...C. = NA, .before = 395)

GAR_raw[390:396,]
# that's better, now the whole converting and clipping procedure again

# GAR_raw$Lokalzeit <- ymd_hms(GAR_raw$Lokalzeit, tz="Europe/Berlin")
# GAR_raw$UTC <- ymd_hms(GAR_raw$UTC, tz="UTC")
# class(GAR_raw$Lokalzeit)
# class(GAR_raw$UTC)


# convert to POSX
GAR_raw$Lokalzeit <- ymd_hms(GAR_raw$Lokalzeit)
GAR_raw$UTC <- ymd_hms(GAR_raw$UTC)
class(GAR_raw$UTC)

Wdat_GAR <- GAR_raw %>%
  filter(., between(Lokalzeit, start_time, end_time))
# this took way too long...




# Freiburg Stadtklimastation
URB_raw <- read.csv("produkt_air_temperature_13667_akt.txt", sep = ";")
# convert POSX
URB_raw$MESS_DATUM <- ymd_h(URB_raw$MESS_DATUM)
# and clip
Wdat_URB <- URB_raw %>% 
  filter(.,between(MESS_DATUM, start_time, end_time)) %>% 
  select(MESS_DATUM, LUFTTEMPERATUR) %>% 
  mutate(dttm = dttm)
# phew..correct nr of observations this time..can deselect orig. timestamp later 
# TODO


# DWD Station 1443
DWD_raw <- read.csv("produkt_tu_stunde_20210718_20230118_01443.txt", sep = ";")
# POSX conversio
DWD_raw$MESS_DATUM <- ymd_h(DWD_raw$MESS_DATUM)
Wdat_DWD <- DWD_raw %>% 
  filter(., between(MESS_DATUM, start_time, end_time)) %>% 
  select(., MESS_DATUM, TT_TU) %>% 
  mutate(dttm = dttm)


# check datetimes
# 
# test <- Wdat_DWD %>% 
#   select(MESS_DATUM, dttm) %>% 
#   mutate(Gartime=Wdat_GAR$Lokalzeit) %>% 
#   mutate(Urbtime=Wdat_URB$MESS_DATUM) %>% 
#   mutate(wbitime=Wdat_WBI)
# jawoll this worked...

# stations df
tempDF <- Wdat_DWD %>% 
  select(., time = dttm, tempDWD = TT_TU) %>% 
  mutate(tempURB = Wdat_URB$LUFTTEMPERATUR) %>% 
  mutate(tempGar = Wdat_GAR$Lufttemperatur...C.) %>% 
  mutate(tempWBI = Wdat_WBI$temp)

# tempWBI still has the comma decimals
tempDF$tempWBI <- scan(text=tempDF$tempWBI, dec = ",", sep = ".")


# my hobo df
# getwd()
myHOBO <- read.csv("10350049_Th.csv" )

tempDF <- tempDF %>% 
  mutate(myHOBO$th)

# # tempDF <- as_tibble(tempDF)
# tempDF$tempWBI <- as.numeric(tempDF$tempWBI)

tempDF <- cbind(tempDF,HOBOtemp = myHOBO$th)
tempDF$HOBOtemp <- as.numeric(tempDF$HOBOtemp)


# manualy zoomed in comparison graph

date1 <- tempDF$time[210]
date2 <- tempDF$time[245]

ggplot(tempDF,aes(time))+
  geom_line(tempDF,mapping=aes(y=tempURB),color = "blue")+
  geom_line(tempDF,mapping=aes(y=tempGar),color = "black")+
  geom_line(tempDF,mapping=aes(y=tempWBI),color = "green")+
  geom_line(tempDF,mapping=aes(y=tempDWD),color = "red")+
  geom_line(tempDF,mapping=aes(y=HOBOtemp),color = "purple")+
  ylim(-5,5)+
  xlim(c(date1,date2))

# tempWBI is the closest fit

ggplot(tempDF,aes(time))+
  #geom_line(tempDF,mapping=aes(y=tempURB),color="blue")+
  #geom_line(tempDF,mapping=aes(y=tempGar),color="black")+
  geom_line(tempDF,mapping=aes(y=tempWBI),color="green")+
  #geom_line(tempDF,mapping=aes(y=tempDWD),color="red")+
  geom_line(tempDF,mapping=aes(y=HOBOtemp),color="purple")
  #ylim(-5,5)+
  #xlim(c(date1,date2))

# TODO plot legends and axis labels, color palette

# MODEL 1: WBI
modWBI <- lm(tempDF$HOBOtemp~tempDF$tempWBI, tempDF)
summary(modWBI)

# MODEL 2: DWD
modDWD <- lm(tempDF$HOBOtemp~tempDF$tempDWD, tempDF)
summary(modDWD)

# MODEL 3: URB
modURB <- lm(tempDF$HOBOtemp~tempDF$tempURB, tempDF)
summary(modURB)

# MODEL 4: GAR
modGAR <- lm(tempDF$HOBOtemp~tempDF$tempGar, tempDF)
summary(modGAR)

# table of model coefficients & R^2

Intercepts <- c(summary(modDWD)$coefficients[1,4],
                  summary(modURB)$coefficients[1,4],
                  summary(modGAR)$coefficients[1,4],
                  summary(modWBI)$coefficients[1,4])

R_squ <- c(summary(modDWD)$r.squared,
           summary(modURB)$r.squared,
           summary(modGAR)$r.squared,
           summary(modWBI)$r.squared)

stations <- c("DWD","URB","GAR","WBI")

MOD_RES <- arrange(tibble(stations,Intercepts,R_squ))

summary(modWBI)
# the WBI model sports the best fit according to R^2
# corroborates what we saw in the plot


```

## 4.2. fill-up

```{r fill regression}

# fill in NA using regression coefficients from the WBI model

predDF <- tibble(HOBOtemp=tempDF$HOBOtemp,tempWBI=tempDF$tempWBI)

preds <- predict(modWBI, tempDF,type="response")

comparison <- tibble(HOBOorig=tempDF$HOBOtemp,
                     tempWBI=tempDF$tempWBI,
                     predicted=preds)

head(comparison)

# make corrected HOBO vector

# create result df

# fill in NAs with model predictions

tempDF <- tempDF %>% 
  mutate(HOBOcorr=case_when(is.na(HOBOtemp)~preds,is.numeric(HOBOtemp)~HOBOtemp))


hobo_hr_corr <- tempDF %>%
  select(dttm=time, th=HOBOcorr) %>%
  mutate(origin="H")


hobo_hr_corr$origin[which(is.na(tempDF$HOBOtemp))] <- "R"


hobo_hr_corr$dttm <- format(hobo_hr_corr$dttm, "%Y-%m-%d %H:%M:%S")
hobo_hr_corr$th <-  format(hobo_hr_corr$th, digits=3, nsmall=3)

write_csv(hobo_hr_corr, file = "10350049_Th.csv" )


```


# 5.	Calculate indices
```{r setup_sql, include=FALSE}
pw <- function () {
  if (Sys.getenv('POSTGRES_PASSWORD') == ""){
    return(getPass('Provide the password: '))
  } else {
    return(Sys.getenv('POSTGRES_PASSWORD'))
  }
}

# establish the connection
drv <- dbDriver('PostgreSQL')
con <- dbConnect(drv, host='hydenv.hydrocode.de', port=5432, user='hydenv', 
                 password=pw(), dbname='hydenv')
```

## 5.1.	Generate metadata overview

<div class="alert alert-info">Generate a *human readable* overview of all HOBOs for this term.</div>


```{R list tables}
dbListTables(con)

```
```{r import meta table}
# get meta table 
HOBO_MTab <- dbReadTable(con, 'metadata')
```

Meta table clipped to this year, location cl parsed using `sf::st_asewkt()`

```{sql connection=con}
SELECT  m.id, device_id as "HOBO id", st_asewkt(location) as location, term_id as term, description
FROM metadata m
WHERE term_id ='15'
```

## 5.2	Calculate Indices

<div class="alert alert-info">Calculate temperature indices for only one timeseries. You can pick any HOBO you like. Needed indices:
* Calculate the mean temperature
* Calculate the mean night temperature
* Calculate the daytime coefficient of variation

Chaged meta_id to that of my own hobo

```{sql connection=con}
SELECT * FROM variables
-- vaiable_id 1 is what I need
```
```{sql connection = con}
select * from raw_data limit 10

```

my HOBO raw data
```{sql connection=con}
create temporary VIEW temperature_index as
SELECT * FROM raw_data 
WHERE meta_id = 216 and variable_id = 1 
```


```{sql connection = con}
SELECT * FROM temperature_index
```



```{sql connection=con}
select AVG(value) as mean from temperature_index

```

now just night
```{sql connection=con}
create temporary VIEW temperature_index_night as
SELECT * FROM raw_data 
WHERE meta_id = 216 and variable_id = 1 and date_part('hour', tstamp) <= 8 and date_part('hour', tstamp) <= 18
```

nightly mean temperature
```{sql connection=con}
select AVG(value) from temperature_index_night
```

and just day

```{sql connection=con}
create temporary VIEW temperature_index_day as
SELECT * FROM raw_data 
WHERE meta_id = 216 and variable_id = 1 and date_part('hour', tstamp) <= 18 and date_part('hour', tstamp) >=8
```

```{sql connection=con}
select * from temperature_index_day
```

and the day mean temp

```{sql connection=con}
select AVG(value) from temperature_index_day
```


## 5.3 Create indices table/view/query

<div class="alert alert-info">
Calculate temperature indices for **all** HOBOs of this term: The indices necessary are:

### create view on this terms hobos temperature

### 5.3.1 this term all hobos

```{sql connection=con}

create temporary VIEW temperature_index_day as
SELECT * FROM raw_data 
WHERE meta_id = 215 and variable_id = 1 and date_part('hour', tstamp) <= 18 and date_part('hour', tstamp) >=8
```


### create view on this terms hobos temperature

```{sql connection=con}

create temporary VIEW temps_232222 as
SELECT * FROM raw_data
WHERE variable_id = 1 and meta_id between 215 and 242

```


Using order by desc (I also checked the first few but there seems to be a HOBO missing) 
```{sql connection=con}

select * from temps_232222
order by meta_id

```

## mean temperature of all this years hobos
*	Mean temperature

```{sql connection=con}

select avg(value) from temps_232222


```

## all hobo day

create view
```{sql connection=con}
create temporary VIEW all_index_day as
SELECT * FROM raw_data 
WHERE variable_id = 1 and meta_id between 215 and 242 and date_part('hour', tstamp) <= 18 and date_part('hour', tstamp) >=8


```

extract daily mean
```{sql connection=con}
select avg(value) from all_index_day
```

## all hobo night
hobo nights sure sounds like a song from the 80s

```{sql connection=con}

create temporary VIEW all_index_night as
SELECT * FROM raw_data 
WHERE variable_id = 1 and meta_id between 215 and 242 and date_part('hour', tstamp) <= 8 and date_part('hour', tstamp) <= 18

```
extract mean

```{sql connection=con}
select avg(value) from all_index_night

```


## 5.4	Switch the dataset (max. 150 words)

<div class="alert alert-info">
Now apply the same query used in the last task again, but for the other kind of data.
</div>
overview of quality controlled data
```{sql connection=con}
create temporary VIEW all_qc_temp as
select * from data 
where variable_id = 1 and
meta_id between 215 and 242

```

## all qc temp mean

```{sql connection=con}
select avg(value) from all_qc_temp

```

## all qc night mean
```{sql connection=con}
create temporary VIEW all_qc_temp_night as
select * from data 
where variable_id = 1 and
meta_id between 215 and 242 and
date_part('hour', tstamp) <= 8 and date_part('hour', tstamp) <= 18
```

```{sql connection=con}
select avg(value) from all_qc_temp_night

```

# all qc day mean
```{sql connection=con}
create temporary VIEW all_qc_temp_day as
select * from data 
where variable_id = 1 and
meta_id between 215 and 242 and
date_part('hour', tstamp) <= 18 and date_part('hour', tstamp) >=8
```

```{sql connection=con}
select avg(value) from all_qc_temp_day
```

## length and coefficient of variation on qc data
### qc coeff.var.
```{sql connection = con}
create temporary view all_qc_varco as
SELECT stddev_samp(value) / AVG(value) from all_qc_temp_day

```
```{sql connection = con}
select * from all_qc_varco
```
### qc length
```{sql connection = con}
create temporary view qc_length2 as
select count(distinct tstamp) from
all_qc_temp
  

```

```{sql connection=con}
select * from qc_length2

```
### raw length
```{sql connection = con}
create temporary view raw_length as
select count(distinct tstamp) from
temps_232222
  

```

```{sql connection=con}
select * from raw_length

```

## funzt nicht

```{sql connection=con}
with c as (
  select m.id, tstamp, value from data d
  join metadata m on m.id=d.meta_id
  join terms t on t.id=m.term_id
  where t.id = 15
),
mean as (
select avg(value) as idx from c

),
idx as (
  select m.id, mean
  from metadata m
  natural join mean
)
select distinct * from idx order by mean
```

<div class="alert alert-info">Disscuss differences between the two tables: Are there differences? Which indices are affected most? Why?</div>

## 5.5 Combine the table with R

<div class="alert alert-info">
In R, calculate **either** of the following indices and finally merge the two tables:
*	Mean time lag between maximum light intensity and maximum temperature
*	Mean of the maximum daily temperature change (per hour)
</div>

First, download the data you want to use:
```{sql connection=con, output.var="my.temp"}
-- Select only your hobo, or directly all you need
SELECT tstamp, value from data where meta_id=150 and variable_id = 1
```
```{r}
# Now, apply the index calculation
calc <- function(df) {
  # your index calculation
}
my.idx <- calc(my.temp)

head(my.temp)
```

Finally, with the index in place, download the indices overview as created in the last two tasks and combine them in R into one overview table:
```{r}
# place your code here  -- you can delete my code if you don't want to use it
postgres.idx <- dbGetQuery(con, 'SELECT * FROM yourview/select statement')
left_join(my.idx, postgres.idx, by=c('meta_id', 'meta_id')) %>%
  View()
```

# 6. Spatial analysis

## 6.1	Find spatial data 
<div class="alert alert-info">You don't need SQL for this task. Check out the table names and do some research online to find out more about the data used.</div>

The data comes from OpenStreetMap, the largest community driven effort to build a database of global structural geodata, which can be used by everyone under a ODbL license, which includes commercial uses. Geodata standards, like the OSM model but also European standards like INSPIRE are extremely important to make public geodata usable. These datasets are heavily structured and their use often involves many dimensions and large data amounts. Without a standard, each authority would publish different data and a collective use is in fact impossible.

## 6.2	Filter by location

<div class="alert alert-info">
Filter the database for only the city districts, that contain a reference station and present them in either in a human readable table or a map.
</div>

First, create a sub-query/view/with statement containing all reference stations. Then extent this with the needed filter
```{sql connection=con}
create temporary view solution_62 as
with ref as (
  select 'DWD 1443' as name, ___ as geom union
  select 'DWD 13667' as name, ___  as geom union
  select 'Uni FR Meteo' as name, ___  as geom union
  select 'WBI' as name, ___ as geom
),
districts as (
  select name, geom from osm_nodes where node_type='district'
)

SELECT d.name as district, ref.name as "reference station", d.geom as geometry 
FROM ___
WHERE ___
```
```{r}
# This is just a suggestion
dist <- read_sf(con, query="SELECT * from solution_62", quiet=T)
plot_ly() %>% 
  add_sf(data=dist, ...) %>%
    layout(
      mapbox=list(style="stamen-terrain", zoom=10., center=list(lon=7.8, lat=48), pitch=15), 
      legend=list(orientation='h'))
```


## 6.3	Aggregate by location 

<div class="alert alert-info">
Query all city districts of Freiburg that contain at least three HOBOs and aggregate the indices calculated in *Create indices table/view/query* for each of these districts and present them as a table.
Repeat the procedure for the HOBO locations of WT21 or WT22 (or both). Are there differences? Describe.
</div>


```{sql connection=con}
with districts as (
  select n.name, count(m.*) as hobos, n.geom from metadata m 
  join osm_nodes n on 
  where term_id = 42 and 
  group by n.name, n.geom
)
select * from districts 
where hobos >= 42
order by hobos desc
```


## 6.4	Creating a map 

<div class="alert alert-info">
Use the queries constructed in the last task to create a map of Freiburg, that illustrates differences in one (or more) of the temperature indices between the city districts of Freiburg. You can create this map either in R or in QGis.
</div>

Get the data here:
```{sql connection=con}

```

Either include the map or make the visualization here:
```{r}

```


# cleanup
```{r}
dbDisconnect(con)
```
